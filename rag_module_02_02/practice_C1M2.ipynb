{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcee1341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "from pprint import pprint as original_pprint\n",
    "import os\n",
    "from together import Together\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57689338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bm25s\n",
      "  Downloading bm25s-0.2.14-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from bm25s) (1.16.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from bm25s) (1.26.4)\n",
      "Downloading bm25s-0.2.14-py3-none-any.whl (55 kB)\n",
      "Installing collected packages: bm25s\n",
      "Successfully installed bm25s-0.2.14\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install bm25s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9563402a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 05:12:32.627201: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-21 05:12:32.789902: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758431552.845307   20242 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758431552.862381   20242 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1758431552.997639   20242 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758431552.997664   20242 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758431552.997665   20242 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758431552.997666   20242 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-21 05:12:33.012847: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI AVX_VNNI_INT8 AVX_NE_CONVERT FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import bm25s\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eea975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(path):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Apply the custom date formatting function to the relevant columns\n",
    "    df['published_at'] = df['published_at'].apply(format_date)\n",
    "    df['updated_at'] = df['updated_at'].apply(format_date)\n",
    "\n",
    "    # Convert the DataFrame to dictionary after formatting\n",
    "    df= df.to_dict(orient='records')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c07a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint(*args, **kwargs):\n",
    "    kwargs.setdefault('sort_dicts', False)\n",
    "    original_pprint(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2847773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_single_input(prompt: str, \n",
    "                               role: str = 'assistant', \n",
    "                               top_p: float = 0, \n",
    "                               temperature: float = 0,\n",
    "                               max_tokens: int = 500,\n",
    "                               model: str =\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "                               together_api_key = None,\n",
    "                              **kwargs):\n",
    "    \n",
    "    if top_p is None:\n",
    "        top_p = 'none'\n",
    "    if temperature is None:\n",
    "        temperature = 'none'\n",
    "\n",
    "    payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": [{'role': role, 'content': prompt}],\n",
    "            \"top_p\": top_p,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            **kwargs\n",
    "                  }\n",
    "    if (not together_api_key) and ('TOGETHER_API_KEY' not in os.environ):\n",
    "        url = os.path.join('https://proxy.dlai.link/coursera_proxy/together', 'v1/chat/completions')   \n",
    "        response = requests.post(url, json = payload, verify=False)\n",
    "        if not response.ok:\n",
    "            raise Exception(f\"Error while calling LLM: f{response.text}\")\n",
    "        try:\n",
    "            json_dict = json.loads(response.text)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to get correct output from LLM call.\\nException: {e}\\nResponse: {response.text}\")\n",
    "    else:\n",
    "        if together_api_key is None:\n",
    "            together_api_key = os.environ['TOGETHER_API_KEY']\n",
    "        client = Together(api_key =  together_api_key)\n",
    "        json_dict = client.chat.completions.create(**payload).model_dump()\n",
    "        json_dict['choices'][-1]['message']['role'] = json_dict['choices'][-1]['message']['role'].name.lower()\n",
    "    try:\n",
    "        output_dict = {'role': json_dict['choices'][-1]['message']['role'], 'content': json_dict['choices'][-1]['message']['content']}\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to get correct output dict. Please try again. Error: {e}\")\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04b07852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance formulas. \n",
    "# In this ungraded lab, distance formulas will be implemented here. However, in future assignments, you will import functions from specialized libraries.\n",
    "def cosine_similarity(v1, array_of_vectors):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between a vector and an array of vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    v1 (array-like): The first vector.\n",
    "    array_of_vectors (array-like): An array of vectors or a single vector.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of cosine similarities between v1 and each vector in array_of_vectors.\n",
    "    \"\"\"\n",
    "    # Ensure that v1 is a numpy array\n",
    "    v1 = np.array(v1)\n",
    "    # Initialize a list to store similarities\n",
    "    similarities = []\n",
    "    \n",
    "    # Check if array_of_vectors is a single vector\n",
    "    if len(np.shape(array_of_vectors)) == 1:\n",
    "        array_of_vectors = [array_of_vectors]\n",
    "    \n",
    "    # Iterate over each vector in the array\n",
    "    for v2 in array_of_vectors:\n",
    "        # Convert the current vector to a numpy array\n",
    "        v2 = np.array(v2)\n",
    "        # Compute the dot product of v1 and v2\n",
    "        dot_product = np.dot(v1, v2)\n",
    "        # Compute the norms of the vectors\n",
    "        norm_v1 = np.linalg.norm(v1)\n",
    "        norm_v2 = np.linalg.norm(v2)\n",
    "        # Compute the cosine similarity and append to the list\n",
    "        similarity = dot_product / (norm_v1 * norm_v2)\n",
    "        similarities.append(similarity)\n",
    "    return np.array(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01848f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def display_widget(llm_call_func, semantic_search_retrieve, bm25_retrieve, reciprocal_rank_fusion):\n",
    "    def on_button_click(b):\n",
    "        query = query_input.value\n",
    "        top_k = slider.value\n",
    "        # Clear existing outputs\n",
    "        for output in [output1, output2, output3, output4]:\n",
    "            output.clear_output()\n",
    "        status_output.clear_output()\n",
    "        # Display \"Generating...\" message\n",
    "        status_output.append_stdout(\"Generating...\\n\")\n",
    "        # Update outputs one by one\n",
    "        results = [\n",
    "            (output1, llm_call_func, query, True, top_k, semantic_search_retrieve),\n",
    "            (output2, llm_call_func, query, True, top_k, bm25_retrieve),\n",
    "            (output3, llm_call_func, query, True, top_k, reciprocal_rank_fusion),\n",
    "            (output4, llm_call_func, query, False, top_k, None)\n",
    "        ]\n",
    "        for output, func, query, use_rag, top_k, retriever in results:\n",
    "            response = func(query=query, use_rag=use_rag, top_k=top_k, retrieve_function=retriever)\n",
    "            with output:\n",
    "                display(Markdown(response))\n",
    "        # Clear \"Generating...\" message\n",
    "        status_output.clear_output()\n",
    "        \n",
    "    query_input = widgets.Text(\n",
    "        description='',\n",
    "        placeholder='Type your query here',\n",
    "        layout=widgets.Layout(width='100%')\n",
    "    )\n",
    "    \n",
    "    slider = widgets.IntSlider(\n",
    "        value=5,\n",
    "        min=1,\n",
    "        max=20,\n",
    "        step=1,\n",
    "        description='Top K:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    output_style = {'border': '1px solid #ccc', 'width': '100%'}\n",
    "    output1 = widgets.Output(layout=output_style)\n",
    "    output2 = widgets.Output(layout=output_style)\n",
    "    output3 = widgets.Output(layout=output_style)\n",
    "    output4 = widgets.Output(layout=output_style)\n",
    "    status_output = widgets.Output()\n",
    "\n",
    "    submit_button = widgets.Button(\n",
    "        description=\"Get Responses\",\n",
    "        style={'button_color': '#eee', 'font_color': 'black'}\n",
    "    )\n",
    "    submit_button.on_click(on_button_click)\n",
    "\n",
    "    label1 = widgets.Label(value=\"Semantic Search\")\n",
    "    label2 = widgets.Label(value=\"BM25 Search\")\n",
    "    label3 = widgets.Label(value=\"Reciprocal Rank Fusion\")\n",
    "    label4 = widgets.Label(value=\"Without RAG\")\n",
    "\n",
    "    display(widgets.HTML(\"\"\"\n",
    "    <style>\n",
    "        .custom-output {\n",
    "            background-color: #f9f9f9;\n",
    "            color: black;\n",
    "            border-radius: 5px;\n",
    "            border: 1px solid #ccc;\n",
    "        }\n",
    "        .widget-text, .widget-button {\n",
    "            background-color: #f0f0f0 !important;\n",
    "            color: black !important;\n",
    "            border: 1px solid #ddd !important;\n",
    "        }\n",
    "        .widget-output {\n",
    "            background-color: #f9f9f9 !important;\n",
    "            color: black !important;\n",
    "        }\n",
    "        input[type=\"text\"] {\n",
    "            background-color: #f0f0f0 !important;\n",
    "            color: black !important;\n",
    "            border: 1px solid #ddd !important;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\"))\n",
    "    \n",
    "    display(query_input, slider, submit_button, status_output)\n",
    "    \n",
    "    # Create individual vertical containers for each label and output\n",
    "    vbox1 = widgets.VBox([label1, output1], layout={'width': '45%'})\n",
    "    vbox2 = widgets.VBox([label2, output2], layout={'width': '45%'})\n",
    "    vbox3 = widgets.VBox([label3, output3], layout={'width': '45%'})\n",
    "    vbox4 = widgets.VBox([label4, output4], layout={'width': '45%'})\n",
    "    \n",
    "    # HBoxes to arrange two VBoxes in each row\n",
    "    hbox_outputs1 = widgets.HBox([vbox1, vbox2], layout={'justify_content': 'space-between'})\n",
    "    hbox_outputs2 = widgets.HBox([vbox3, vbox4], layout={'justify_content': 'space-between'})\n",
    "\n",
    "    def style_outputs(*outputs):\n",
    "        for output in outputs:\n",
    "            output.layout.margin = '5px'\n",
    "            output.layout.height = '300px'\n",
    "            output.layout.padding = '10px'\n",
    "            output.layout.overflow = 'auto'\n",
    "            output.add_class(\"custom-output\")\n",
    "            \n",
    "    style_outputs(output1, output2, output3, output4)\n",
    "    \n",
    "    # Display two rows with two outputs each\n",
    "    display(hbox_outputs1)\n",
    "    display(hbox_outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51eb4df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_date(date_string):\n",
    "    # Parse the input string into a datetime object\n",
    "    date_object = parser.parse(date_string)\n",
    "    # Format the date to \"YYYY-MM-DD\"\n",
    "    formatted_date = date_object.strftime(\"%Y-%m-%d\")\n",
    "    return formatted_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ff13275",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_DATA = read_dataframe(\"news_data_dedup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d05669c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'guid': '18ba9f2676859f393a271d15692a9c6e',\n",
      " 'title': 'WATCH: Would you pay a tourist fee to enter Venice?',\n",
      " 'description': 'From Thursday visitors making a trip to the famous city at '\n",
      "                'peak times will be charged a trial entrance fee.',\n",
      " 'venue': 'BBC',\n",
      " 'url': 'https://www.bbc.co.uk/news/world-europe-68898441',\n",
      " 'published_at': '2024-04-25',\n",
      " 'updated_at': '2024-04-26'}\n"
     ]
    }
   ],
   "source": [
    "pprint(NEWS_DATA[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "551554b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_news(indices):\n",
    "    \"\"\"\n",
    "    Retrieves elements from a dataset based on specified indices.\n",
    "\n",
    "    Parameters:\n",
    "    indices (list of int): A list containing the indices of the desired elements in the dataset.\n",
    "    dataset (list or sequence): The dataset from which elements are to be retrieved. It should support indexing.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of elements from the dataset corresponding to the indices provided in list_of_indices.\n",
    "    \"\"\"\n",
    "     \n",
    "    output = [NEWS_DATA[index] for index in indices]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee7dcfc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e19f085d05a427982e37f6c84b1c12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b47d88ae7ff403b841210b4563353b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Count Tokens:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfce59f914a646f0a89b9adac01d9be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Compute Scores:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d8fd885b274946a80a4f38aeedd710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91286b425f544d1babe3ee4b43736f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for query: What are the recent news about GDP?\n",
      "\n",
      "Document retrieved 752 : GDP and the Dow Are Up. But What About American Well-Being? The standard ways of measuring economic growth don’t capture what life is like for real people. A new metric offers a better alternative, especially for seeing disparities across the country.\n",
      "\n",
      "Document retrieved 673 : What the GDP Report Says About Inflation: A Hot First Quarter Thursday’s gross domestic product report suggests that a widely watched inflation reading due Friday could be worse than expected.\n",
      "\n",
      "Document retrieved 289 : A GDP Warning as Signs of Stagflation Appear Slower growth and persistent inflation explain why voters feel glum about the economy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The corpus used will be the title appended with the description\n",
    "corpus = [x['title'] + \" \" + x['description'] for x in NEWS_DATA]\n",
    "\n",
    "# Instantiate the retriever by passing the corpus data\n",
    "BM25_RETRIEVER = bm25s.BM25(corpus=corpus)\n",
    "\n",
    "# Tokenize the chunks\n",
    "tokenized_data = bm25s.tokenize(corpus)\n",
    "\n",
    "# Index the tokenized chunks within the retriever\n",
    "BM25_RETRIEVER.index(tokenized_data)\n",
    "\n",
    "# Tokenize the same query used in the previous exercise\n",
    "sample_query = \"What are the recent news about GDP?\"\n",
    "tokenized_sample_query = bm25s.tokenize(sample_query)\n",
    "\n",
    "# Get the retrieved results and their respective scores\n",
    "results, scores = BM25_RETRIEVER.retrieve(tokenized_sample_query, k=3)\n",
    "\n",
    "print(f\"Results for query: {sample_query}\\n\")\n",
    "for doc in results[0]:\n",
    "  print(f\"Document retrieved {corpus.index(doc)} : {doc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "994f32c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95307caa2a204bcc8c66dcd008ba84a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41524634545d4d888a7db5f1f52b0373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Count Tokens:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72774d66f14a48c19ccd4cb4b25fa8ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Compute Scores:   0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use these as a global defined BM25 retriever objects\n",
    "\n",
    "corpus = [x['title'] + \" \" + x['description'] for x in NEWS_DATA]\n",
    "BM25_RETRIEVER = bm25s.BM25(corpus=corpus)\n",
    "TOKENIZED_DATA = bm25s.tokenize(corpus)\n",
    "BM25_RETRIEVER.index(TOKENIZED_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82981a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED CELL\n",
    "\n",
    "def bm25_retrieve(query: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Retrieves the top k relevant documents for a given query using the BM25 algorithm.\n",
    "\n",
    "    This function tokenizes the input query and uses a pre-indexed BM25 retriever to\n",
    "    search through a collection of documents. It returns the indices of the top k documents\n",
    "    that are most relevant to the query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query for which documents need to be retrieved.\n",
    "        top_k (int): The number of top relevant documents to retrieve. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: A list of indices corresponding to the top k relevant documents\n",
    "        within the corpus.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Tokenize the query using the 'tokenize' function from the 'bm25s' module\n",
    "    tokenized_query = bm25s.tokenize(query)\n",
    "    \n",
    "    # Use the 'BM25_RETRIEVER' to retrieve documents and their scores based on the tokenized query\n",
    "    # Retrieve the top 'k' documents\n",
    "    results, scores = BM25_RETRIEVER.retrieve(tokenized_query, k=top_k)\n",
    "\n",
    "    # Extract the first element from 'results' to get the list of retrieved documents\n",
    "    results = results[0]\n",
    "\n",
    "    # Convert the retrieved documents into their corresponding indices in the results list\n",
    "    top_k_indices = [corpus.index(result) for result in results]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79546a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80dec3e6e0bb4234b293824da8ce6ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ef8ee6dd344144b7ed52ebaaf79439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[752, 673, 289, 626, 43]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output is a list of indices\n",
    "bm25_retrieve(\"What are the recent news about GDP?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20834654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-computed embeddings with joblib\n",
    "EMBEDDINGS = joblib.load(\"embeddings.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee9ff086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('BAAI/bge-base-en-v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cddcd9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00886308, -0.04775141, -0.00156084,  0.01309998, -0.00206938,\n",
       "       -0.06157261,  0.01384687,  0.0010149 , -0.04903951, -0.04762556,\n",
       "       -0.03628187,  0.00478034, -0.0349218 ,  0.05323153,  0.02193961,\n",
       "        0.03645134,  0.04029364, -0.00453638,  0.01883798, -0.03367381,\n",
       "        0.02516189, -0.0484363 , -0.04047947,  0.02590901,  0.02175235,\n",
       "        0.03160365,  0.03937933, -0.03640462, -0.03113292, -0.01247228,\n",
       "        0.03661649, -0.00458203, -0.00100168, -0.0318879 ,  0.02957138,\n",
       "        0.01986158, -0.0073747 ,  0.02370171, -0.02151621, -0.07361359],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"RAG is awesome\"\n",
    "# Using, but truncating the result to not pollute the output, don't truncate it in the exercise.\n",
    "model.encode(query)[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bc72684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'What are the primary colors' and 'Yellow, red and blue' = 0.7377139329910278\n",
      "Similarity between 'What are the primary colors' and 'Cats are friendly animals' = 0.4508620500564575\n"
     ]
    }
   ],
   "source": [
    "query1 = \"What are the primary colors\"\n",
    "query2 = \"Yellow, red and blue\"\n",
    "query3 = \"Cats are friendly animals\"\n",
    "\n",
    "query1_embed = model.encode(query1)\n",
    "query2_embed = model.encode(query2)\n",
    "query3_embed = model.encode(query3)\n",
    "\n",
    "print(f\"Similarity between '{query1}' and '{query2}' = {cosine_similarity(query1_embed, query2_embed)[0]}\")\n",
    "print(f\"Similarity between '{query1}' and '{query3}' = {cosine_similarity(query1_embed, query3_embed)[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b226e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350 176]\n"
     ]
    }
   ],
   "source": [
    "query = \"Taylor Swift\"\n",
    "query_embed = model.encode(query)\n",
    "# The result is a matrix with one matrix per sample. Since there is only one sample (the query), it is a matrix with one matrix within.\n",
    "# This is why you need to get the first element\n",
    "similarity_scores = cosine_similarity(query_embed, EMBEDDINGS)\n",
    "similarity_indices = np.argsort(-similarity_scores) # Sort on decreasing order (sort the negative on increasing order), but return the indices\n",
    "# Top 2 indices\n",
    "top_2_indices = similarity_indices[:2]\n",
    "print(top_2_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90f85e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'guid': '927257674585bb6ef669cf2c2f409fa7',\n",
       "  'title': '‘The working class can’t afford it’: the shocking truth about the money bands make on tour',\n",
       "  'description': 'As Taylor Swift tops $1bn in tour revenue, musicians playing smaller venues are facing pitiful fees and frequent losses. Should the state step in to save our live music scene?When you see a band playing to thousands of fans in a sun-drenched festival field, signing a record deal with a major label or playing endlessly from the airwaves, it’s easy to conjure an image of success that comes with some serious cash to boot – particularly when Taylor Swift has broken $1bn in revenue for her current Eras tour. But looks can be deceiving. “I don’t blame the public for seeing a band playing to 2,000 people and thinking they’re minted,” says artist manager Dan Potts. “But the reality is quite different.”Post-Covid there has been significant focus on grassroots music venues as they struggle to stay open. There’s been less focus on the actual ability of artists to tour these venues. David Martin, chief executive officer of the Featured Artists Coalition (FAC), says we’re in a “cost-of-touring crisis”. Pretty much every cost attached to touring – van hire, crew, travel, accommodation, food and drink – has gone up, while fees and audiences often have not. “[Playing] live is becoming financially unsustainable for many artists,” he says. “Artists are seeing [playing] live as a loss leader now. That’s if they can even afford to make it work in the first place.” Continue reading...',\n",
       "  'venue': 'The Guardian',\n",
       "  'url': 'https://www.theguardian.com/music/2024/apr/25/shocking-truth-money-bands-make-on-tour-taylor-swift',\n",
       "  'published_at': '2024-04-25',\n",
       "  'updated_at': '2024-04-26'},\n",
       " {'guid': 'e8712c1d69d29f0ca802f07d9b60ffe7',\n",
       "  'title': 'Estate of Tupac Shakur threatens legal action against Drake over AI diss track',\n",
       "  'description': 'Drake used AI to simulate the voice of the late rapper and have him chide Kendrick Lamar, which the estate calls a ‘flagrant violation’The estate of the late Tupac Shakur has sent a cease and desist letter to Drake, following the release of a Drake track that uses an AI version of Shakur’s voice to lambast Kendrick Lamar.As seen by Billboard, the letter instructs Drake to remove the track, Taylor Made Freestyle, within 24 hours, or face legal action. Continue reading...',\n",
       "  'venue': 'The Guardian',\n",
       "  'url': 'https://www.theguardian.com/music/2024/apr/25/estate-of-tupac-shakur-threatens-legal-action-against-drake-over-ai-diss-track',\n",
       "  'published_at': '2024-04-25',\n",
       "  'updated_at': '2024-04-26'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieving the data\n",
    "query_news(top_2_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16abebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED CELL \n",
    "\n",
    "def semantic_search_retrieve(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieves the top k relevant documents for a given query using semantic search and cosine similarity.\n",
    "\n",
    "    This function generates an embedding for the input query and compares it against pre-computed document\n",
    "    embeddings using cosine similarity. The indices of the top k most similar documents are returned.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query for which relevant documents need to be retrieved.\n",
    "        top_k (int): The number of top relevant documents to retrieve. Default value is 5.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: A list of indices corresponding to the top k most relevant documents in the corpus.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    # Generate the embedding for the query using the pre-trained model\n",
    "    query_embedding = model.encode(query)\n",
    "    \n",
    "    # Calculate the cosine similarity scores between the query embedding and the pre-computed document embeddings\n",
    "    similarity_scores = cosine_similarity(query_embedding, EMBEDDINGS)\n",
    "    \n",
    "    # Sort the similarity scores in descending order and get the indices\n",
    "    similarity_indices = np.argsort(-similarity_scores)\n",
    "\n",
    "    # Select the indices of the top k documents as a numpy array\n",
    "    top_k_indices_array = similarity_indices[:top_k]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cast them to int \n",
    "    top_k_indices = [int(x) for x in top_k_indices_array]\n",
    "    \n",
    "    return top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c22f6416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[743, 673, 626, 752, 326]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see an example\n",
    "semantic_search_retrieve(\"What are the recent news about GDP?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6da8660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED CELL \n",
    "def reciprocal_rank_fusion(list1, list2, top_k=5, K=60):\n",
    "    \"\"\"\n",
    "    Fuse rank from multiple IR systems using Reciprocal Rank Fusion.\n",
    "\n",
    "    Args:\n",
    "        list1 (list[int]): A list of indices of the top-k documents that match the query.\n",
    "        list2 (list[int]): Another list of indices of the top-k documents that match the query.\n",
    "        top_k (int): The number of top documents to consider from each list for fusion. Defaults to 5.\n",
    "        K (int): A constant used in the RRF formula. Defaults to 60.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: A list of indices of the top-k documents sorted by their RRF scores.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Create a dictionary to store the RRF scores for each document index\n",
    "    rrf_scores = {}\n",
    "\n",
    "    # Iterate over each document list\n",
    "    for lst in [list1, list2]:\n",
    "        # Calculate the RRF score for each document index\n",
    "        for rank, item in enumerate(lst, start=1): # Start = 1 set the first element as 1 and not 0. \n",
    "                                                   # This is a convention on how ranks work (the first element in ranking is denoted by 1 and not 0 as in lists)\n",
    "            # If the item is not in the dictionary, initialize its score to 0\n",
    "            if item not in rrf_scores:\n",
    "                rrf_scores[item] = 0\n",
    "            # Update the RRF score for each document index using the formula 1 / (rank + K)\n",
    "            rrf_scores[item] += (1 / (rank + K))\n",
    "\n",
    "    # Sort the document indices based on their RRF scores in descending order\n",
    "    sorted_items = sorted(rrf_scores, key=rrf_scores.get, reverse = True)\n",
    "\n",
    "    # Slice the list to get the top-k document indices\n",
    "    top_k_indices = [int(x) for x in sorted_items[:top_k]]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a791f735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf757457f634174bb599b19d3ff4202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0674168c37d4059bda44c2b4900a28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search List: [743, 673, 626, 752, 326]\n",
      "BM25 List: [752, 673, 289, 626, 43]\n",
      "RRF List: [673, 752, 626, 743, 289]\n"
     ]
    }
   ],
   "source": [
    "list1 = semantic_search_retrieve('What are the recent news about GDP?')\n",
    "list2 = bm25_retrieve('What are the recent news about GDP?')\n",
    "rrf_list = reciprocal_rank_fusion(list1, list2)\n",
    "print(f\"Semantic Search List: {list1}\")\n",
    "print(f\"BM25 List: {list2}\")\n",
    "print(f\"RRF List: {rrf_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6dc1bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_prompt(query, top_k, retrieve_function = None, use_rag=True):\n",
    "    \"\"\"\n",
    "    Generates an augmented prompt for a Retrieval-Augmented Generation (RAG) system by retrieving the top_k most \n",
    "    relevant documents based on a given query.\n",
    "\n",
    "    Parameters:\n",
    "    query (str): The search query for which the relevant documents are to be retrieved.\n",
    "    top_k (int): The number of top relevant documents to retrieve.\n",
    "    retrieve_function (callable): The function used to retrieve relevant documents. If 'reciprocal_rank_fusion', \n",
    "                                  it will combine results from different retrieval functions.\n",
    "    use_rag (bool): A flag to determine whether to incorporate retrieved data into the prompt (default is True).\n",
    "\n",
    "    Returns:\n",
    "    str: A prompt that includes the top_k relevant documents formatted for use in a RAG system.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the prompt as the initial query\n",
    "    prompt = query\n",
    "    \n",
    "    # If not using rag, return the prompt\n",
    "    if not use_rag:\n",
    "        return prompt\n",
    "\n",
    "\n",
    "    # Determine which retrieve function to use based on its name.\n",
    "    if retrieve_function.__name__ == 'reciprocal_rank_fusion':\n",
    "        # Retrieve top documents using two different methods.\n",
    "        list1 = semantic_search_retrieve(query, top_k)\n",
    "        list2 = bm25_retrieve(query, top_k)\n",
    "        # Combine the results using reciprocal rank fusion.\n",
    "        top_k_indices = retrieve_function(list1, list2, top_k)\n",
    "    else:\n",
    "        # Use the provided retrieval function.\n",
    "        top_k_indices = retrieve_function(query=query, top_k=top_k)\n",
    "    \n",
    "    \n",
    "    # Retrieve documents from the dataset using the indices.\n",
    "    relevant_documents = query_news(top_k_indices)\n",
    "    \n",
    "    formatted_documents = []\n",
    "\n",
    "    # Iterate over each retrieved document.\n",
    "    for document in relevant_documents:\n",
    "        # Format each document into a structured string.\n",
    "        formatted_document = (\n",
    "            f\"Title: {document['title']}, Description: {document['description']}, \"\n",
    "            f\"Published at: {document['published_at']}\\nURL: {document['url']}\"\n",
    "        )\n",
    "        # Append the formatted string to the main data string with a newline for separation.\n",
    "        formatted_documents.append(formatted_document)\n",
    "\n",
    "    retrieve_data_formatted = \"\\n\".join(formatted_documents)\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Answer the user query below. There will be provided additional information for you to compose your answer. \"\n",
    "        f\"The relevant information provided is from 2024 and it should be added as your overall knowledge to answer the query, \"\n",
    "        f\"you should not rely only on this information to answer the query, but add it to your overall knowledge.\"\n",
    "        f\"Query: {query}\\n\"\n",
    "        f\"2024 News: {retrieve_data_formatted}\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62736f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOGETHER_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "423f6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_single_input(prompt: str, \n",
    "                               role: str = 'assistant', \n",
    "                               top_p: float = 0, \n",
    "                               temperature: float = 0,\n",
    "                               max_tokens: int = 500,\n",
    "                               model: str =\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "                               together_api_key = None,\n",
    "                              **kwargs):\n",
    "    \n",
    "    if top_p is None:\n",
    "        top_p = 'none'\n",
    "    if temperature is None:\n",
    "        temperature = 'none'\n",
    "\n",
    "    payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": [{'role': role, 'content': prompt}],\n",
    "            \"top_p\": top_p,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            **kwargs\n",
    "                  }\n",
    "    if (not together_api_key) and ('TOGETHER_API_KEY' not in os.environ):\n",
    "        url = os.path.join('https://proxy.dlai.link/coursera_proxy/together', 'v1/chat/completions')   \n",
    "        response = requests.post(url, json = payload, verify=False)\n",
    "        if not response.ok:\n",
    "            raise Exception(f\"Error while calling LLM: f{response.text}\")\n",
    "        try:\n",
    "            json_dict = json.loads(response.text)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to get correct output from LLM call.\\nException: {e}\\nResponse: {response.text}\")\n",
    "    else:\n",
    "        if together_api_key is None:\n",
    "            together_api_key = os.environ['TOGETHER_API_KEY']\n",
    "        client = Together(api_key =  together_api_key)\n",
    "        json_dict = client.chat.completions.create(**payload).model_dump()\n",
    "        json_dict['choices'][-1]['message']['role'] = json_dict['choices'][-1]['message']['role'].name.lower()\n",
    "    try:\n",
    "        output_dict = {'role': json_dict['choices'][-1]['message']['role'], 'content': json_dict['choices'][-1]['message']['content']}\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to get correct output dict. Please try again. Error: {e}\")\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb002192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(query, retrieve_function = None, top_k = 5,use_rag = True):\n",
    "\n",
    "    # Get the system and user dictionaries\n",
    "    prompt = generate_final_prompt(query, top_k = top_k, retrieve_function = retrieve_function, use_rag = use_rag)\n",
    "\n",
    "    generated_response = generate_with_single_input(prompt)\n",
    "\n",
    "    generated_message = generated_response['content']\n",
    "    \n",
    "    return generated_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c757fe95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There have been several recent news in the technology sector. Some of the key developments include:\n",
      "\n",
      "1. **Artificial Intelligence Sparks 'Game of Thrones' in the Chip Industry**: The rapid advancement of artificial intelligence (AI) is transforming the semiconductor industry, creating new winners and losers. The supply chain is becoming increasingly complex, with various players vying for dominance. (Source: El Pais, April 12, 2024)\n",
      "\n",
      "2. **Tech Spending Still Proves Thorny for Some Advertising Companies**: The technology sector's slower pace of business has continued to affect some advertising holding companies in the first quarter. However, there are signs that things might be looking up. (Source: The Wall Street Journal, April 26, 2024)\n",
      "\n",
      "3. **Market Talks: T-Mobile, Imax, Rogers Communications, and More**: The latest Market Talks covering Technology, Media, and Telecom have highlighted key developments in the sector, including updates on T-Mobile, Imax, and Rogers Communications. (Source: The Wall Street Journal, April 25, 2024)\n",
      "\n",
      "4. **Market Talks: China Telecom, Bilibili, and More**: The latest Market Talks covering Technology, Media, and Telecom have also highlighted key developments in the sector, including updates on China Telecom and Bilibili. (Source: The Wall Street Journal, April 26, 2024)\n",
      "\n",
      "These news stories demonstrate the ongoing evolution of the technology sector, with AI, advertising, and market trends shaping the industry.\n",
      "\n",
      "Sources:\n",
      "- El Pais: https://english.elpais.com/technology/2024-04-12/artificial-intelligence-sparks-game-of-thrones-in-the-chip-industry.html\n",
      "- The Wall Street Journal: \n",
      "  - https://www.wsj.com/articles/tech-media-telecom-roundup-market-talk-c2ae6c7a\n",
      "  - https://www.wsj.com/articles/tech-spending-still-proves-thorny-for-some-advertising-companies-5d8216f2\n",
      "  - https://www.wsj.com/articles/tech-media-telecom-roundup-market-talk-f4376a81\n"
     ]
    }
   ],
   "source": [
    "query = \"Recent news in technology. Provide sources.\"\n",
    "print(llm_call(query, retrieve_function = semantic_search_retrieve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002d3e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfafe7df4f44b339725485ba52540a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n    <style>\\n        .custom-output {\\n            background-color: #f9f9f9;\\n            color…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee22ca58db844ab97d170b0dfc58f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', layout=Layout(width='100%'), placeholder='Type your query here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab1ac1da9ed4685850fae4bdd55905d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=5, description='Top K:', max=20, min=1, style=SliderStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ab70b1784a4d7abe94eee769326d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Get Responses', style=ButtonStyle(button_color='#eee'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4eea0027f3468ca61c88c78309d77d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_widget(llm_call, semantic_search_retrieve, bm25_retrieve, reciprocal_rank_fusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
