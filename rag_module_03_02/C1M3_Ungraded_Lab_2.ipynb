{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9eb6137-9072-4f41-8e04-bb3031bc1f39",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ungraded Lab: Chunking\n",
    "---\n",
    "Welcome to the Ungraded Lab on Chunking! As you saw in the lectures, chunking breaks large texts into smaller, manageable pieces, which is essential for efficiently working with vector databases and language models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88ad84d",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "- [ 1 - Introduction](#1)\n",
    "  - [ 1.1 Importing necessary libraries](#1-1)\n",
    "  - [ 1.2 Downloading the data](#1-2)\n",
    "- [ 2 - Fixed-size chunking](#2)\n",
    "  - [ 2.1 Example Chunking Code](#2-1)\n",
    "  - [ 2.2 Chunking with overlap](#2-2)\n",
    "- [ 3 - Variable-size chunking - Recursive Character Splitting](#3)\n",
    "  - [ 3.1 Pseudo-code for variable-size chunking methods](#3-1)\n",
    "  - [ 3.2 Mixing fixed and variable-sized chunking](#3-2)\n",
    "- [ 4 - Chunking on real data](#4)\n",
    "  - [ 4.1 Getting the data](#4-1)\n",
    "  - [ 4.2 Chunking the chapters](#4-2)\n",
    "  - [ 4.3 Loading Chunks into a Vector Database](#4-3)\n",
    "- [ 5 - Searching ](#5)\n",
    "- [ 6 - Incorporating in a RAG system](#6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4752a252-e274-4028-b67d-bbc1e1a83950",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1 - Introduction\n",
    "\n",
    "---\n",
    "\n",
    "Chunking plays an important role in information retrieval. For example, when building a vector database from a collection of books, different chunk sizes can serve different purposes. Cataloging entire books as single vectors may help in identifying broad themes, but misses specific details. Chunking closer to the paragraph or sentence level enables the retrieval of specific information or concepts.\n",
    "\n",
    "Language models typically have limitations on the amount of text they can process at once, known as the \"context window.\" Chunking helps ensure that text inputs remain within these boundaries, allowing models to handle large documents, like novels, by splitting them into smaller sections.\n",
    "\n",
    "In this ungraded lab you will explore ways of chunking and see how it can impact RAG systems!\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/chunking.png\" alt=\"Overview\" width=\"80%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050dedb3-cbdd-4d59-9f66-1bac81d22a81",
   "metadata": {},
   "source": [
    "<a id='1-1'></a>\n",
    "### 1.1 Importing necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e848578-2a9a-4149-a1e2-e996517234fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import requests\n",
    "import re\n",
    "import weaviate\n",
    "from weaviate.classes.config import Configure, Property, DataType, Tokenization\n",
    "from weaviate.util import generate_uuid5\n",
    "import tqdm\n",
    "from weaviate.classes.query import Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10877f26-583a-4729-95e1-fef3fdbe2a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import flask_app\n",
    "from utils import (\n",
    "    generate_with_single_input, \n",
    "    suppress_subprocess_output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c71b6ea-18f3-4d99-950a-d99cda36dd0c",
   "metadata": {},
   "source": [
    "<a id='1-2'></a>\n",
    "### 1.2 Downloading the data\n",
    "\n",
    "Now you need some text long enough to justify chunking. Let's take a part from the [Pro Git book](https://git-scm.com/book/en/v2) a specifically a chapter called \"What is Git?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e523dee-d57c-4feb-b706-3f3789cba75b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/progit/progit2/main/book/01-introduction/sections/what-is-git.asc\"\n",
    "source_text = requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ffdac-7fe4-4a65-94bb-44d0b11f53d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(source_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0cccc1-9b44-4dbc-a08a-cad0be330635",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"There are about {len(source_text.split())} words in this chapter. Depending on how your LLM tokenizes words, you'd expect roughly {round(len(source_text.split())*1.3)} tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cab1c42-f8da-47ad-b540-c5a5a170c409",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2 - Fixed-size chunking\n",
    "---\n",
    "Fixed-size chunking means breaking texts into pieces of the same size. For example, you might split an article into parts of 100 words each or sections of 200 characters each. This method is common because it is easy to use and works well.\n",
    "\n",
    "It works by dividing texts into pieces that have a set number of units. These units can be words, characters, or even tokens. The number of units in each piece is the same up to a maximum limit, and there can be an optional overlap between the pieces.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/fixed_size.png\" alt=\"Fixed Size Chunking\" width=\"80%\">\n",
    "</div>\n",
    "\n",
    "<a id='2-1'></a>\n",
    "### 2.1 Example Chunking Code\n",
    "\n",
    "Let's see now an implementation for fixed-size chunking. There are many different implementations. The following implementation is a possible one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce512d1e-b31c-4b6d-9025-4d21894fcb78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_chunks_fixed_size(text: str, chunk_size: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits a given text into chunks of a specified fixed size.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be split into chunks.\n",
    "        chunk_size (int): The maximum number of words per chunk.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of text chunks, each containing up to 'chunk_size' words.\n",
    "    \"\"\"\n",
    "    # Split the input text into individual words\n",
    "    text_words = text.split()\n",
    "    \n",
    "    # Initialize a list to hold the chunks of words\n",
    "    chunks = []\n",
    "    \n",
    "    # Iterate over the word indices in steps of 'chunk_size'\n",
    "    for i in range(0, len(text_words), chunk_size):\n",
    "        # Select a sublist of words from 'i' to 'i + chunk_size'\n",
    "        chunk_words = text_words[i: i + chunk_size]\n",
    "        \n",
    "        # Join the selected words into a single string with spaces in between\n",
    "        chunk = \" \".join(chunk_words)\n",
    "        \n",
    "        # Add the chunk to the list of chunks\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    # Return the list of word chunks\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc020e1b-693f-4d96-b4b2-c0beb7298f5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fixed_size_chunks = get_chunks_fixed_size(source_text, chunk_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a270aec6-4256-4f78-91b6-7c7f5d73c199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(fixed_size_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895ba773-d1a6-417d-a7e9-01deb39e81d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fixed_size_chunks[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5577a53-afe9-46d3-8314-780af8d7b056",
   "metadata": {},
   "source": [
    "<a id='2-2'></a>\n",
    "### 2.2 Chunking with overlap\n",
    "\n",
    "Let's modify the code to allow overlapping, so chunks will have shared tokens.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/overlap.png\" alt=\"Chunking with overlap\" width=\"80%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a695905d-64dd-47ad-883e-2c461db178ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_chunks_fixed_size_with_overlap(text: str, chunk_size: int, overlap_fraction: float) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits a given text into chunks of a fixed size with a specified overlap fraction between consecutive chunks.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text to be split into chunks.\n",
    "    - chunk_size (int): The number of words each chunk should contain.\n",
    "    - overlap_fraction (float): The fraction of the chunk size that should overlap with the adjacent chunk.\n",
    "      For example, an overlap_fraction of 0.2 means 20% of the chunk size will be used as overlap.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list of chunks (each a string) where each chunk might overlap with its adjacent chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the text into individual words\n",
    "    text_words = text.split()\n",
    "    \n",
    "    # Calculate the number of words to overlap between consecutive chunks\n",
    "    overlap_int = int(chunk_size * overlap_fraction)\n",
    "    \n",
    "    # Initialize a list to store the resulting chunks\n",
    "    chunks = []\n",
    "    \n",
    "    # Iterate over text in steps of chunk_size to create chunks\n",
    "    for i in range(0, len(text_words), chunk_size):\n",
    "        # Determine the start and end indices for the current chunk,\n",
    "        # taking into account the overlap with the previous chunk\n",
    "        chunk_words = text_words[max(i - overlap_int, 0): i + chunk_size]\n",
    "        \n",
    "        # Join the selected words to form a chunk string\n",
    "        chunk = \" \".join(chunk_words)\n",
    "        \n",
    "        # Append the chunk to the list of chunks\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    # Return the list of chunks\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4be25c-f99b-4c30-8e1b-b8119fa143ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for chosen_size in [5, 25, 100]:\n",
    "    chunks = get_chunks_fixed_size_with_overlap(source_text, chosen_size, overlap_fraction=0.2)\n",
    "    # Print outputs to screen\n",
    "    print(f\"\\nSize {chosen_size} - {len(chunks)} chunks returned.\")\n",
    "    for i in range(3):\n",
    "        print(f\"Chunk {i+1}: {chunks[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5b1257-6cf2-42fc-ae45-be0abb2aa1b8",
   "metadata": {},
   "source": [
    "Note that the smaller chunks of text are very detailed, but they might **not have enough information to be useful for searching**. In contrast, **larger chunks start to contain more information, similar to a typical paragraph in length**. As these chunks become even longer, **their associated vector embeddings become more general**. Eventually, they reach a point where they are no longer effective for information searching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50e5ff3-7db6-4af5-982b-e2fa022f4a8d",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3 - Variable-size chunking - Recursive Character Splitting\n",
    "\n",
    "---\n",
    "Now let's examine variable-size chunking. Unlike fixed-size chunking, the size of each chunk here is a result, not a starting point. In variable-size chunking, text is divided using a specific marker. This marker could be something like a sentence or paragraph break or even a structural element like a markdown header.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/recursive.png\" alt=\"Recursive Character Splitting\" width=\"80%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3c7c86-a1e5-4999-a5ba-f0b12d7ddce2",
   "metadata": {},
   "source": [
    "<a id='3-1'></a>\n",
    "### 3.1 Pseudo-code for variable-size chunking methods\n",
    "\n",
    "The simplest one is to split into paragraphs (`\\n\\n`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb9ae4a-ef05-4449-b336-35d77ffff8c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the text into paragraphs\n",
    "def get_chunks_by_paragraph(source_text: str) -> List[str]:\n",
    "    return source_text.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654edb37-1654-4f30-84c6-b2488a195665",
   "metadata": {},
   "source": [
    "Another way, in this context, is to split into sections. As you can see inspecting the text, sections are divided with `\\n==` markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4a2bc1-2b2e-42dd-b466-46a155b4c4a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the text by Asciidoc section markers\n",
    "def get_chunks_by_asciidoc_sections(source_text: str) -> List[str]:\n",
    "    return source_text.split(\"\\n==\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668c60f7-7314-4ffe-82e4-b052018f0c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for marker in [\"\\n\\n\", \"\\n==\"]:\n",
    "    chunks = source_text.split(marker)\n",
    "    # Print outputs to screen\n",
    "    print(f\"\\nUsing the marker: {repr(marker)} - {len(chunks)} chunks returned.\")\n",
    "    for i in range(3):\n",
    "        print(f\"Chunk {i+1}: {repr(chunks[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8cca7a-e188-4d6b-94eb-af81025dfa29",
   "metadata": {},
   "source": [
    "One noticeable issue with simple marker-based chunking is that **headings often become separate chunks**, which might not be ideal. In practice, you might use a mixed strategy by attaching short chunks, like headings, to the following chunk. This way, the heading stays connected to its relevant section. Let's explore this approach further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ebff1c-6d84-4135-98df-ab13302e5a87",
   "metadata": {},
   "source": [
    "<a id='3-2'></a>\n",
    "### 3.2 Mixing fixed and variable-sized chunking\n",
    "\n",
    "You can combine fixed-size and variable-size chunking to take advantage of both methods. For instance, use a variable-size chunker to divide text at paragraph markers, and then apply a fixed-size filter. If a chunk is too small, you can merge it with the next one, and if a chunk is too large, you can split it in the middle or at another marker within the chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b770d41-3a7f-451f-8ab7-a54d05054105",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mixed_chunking(source_text):\n",
    "    \"\"\"\n",
    "    Splits the given source_text into chunks using a mix of fixed-size and variable-size chunking.\n",
    "    It first splits the text by Asciidoc markers and then processes the chunks to ensure they are \n",
    "    of appropriate size. Smaller chunks are merged with the next chunk, and larger chunks can be \n",
    "    further split at the middle or specific markers within the chunk.\n",
    "\n",
    "    Args:\n",
    "    - source_text (str): The text to be chunked.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of text chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the text by Asciidoc marker\n",
    "    chunks = source_text.split(\"\\n==\")\n",
    "\n",
    "    # Chunking logic\n",
    "    new_chunks = []\n",
    "    chunk_buffer = \"\"\n",
    "    min_length = 25\n",
    "\n",
    "    for chunk in chunks:\n",
    "        new_buffer = chunk_buffer + chunk  # Create new buffer\n",
    "        new_buffer_words = new_buffer.split(\" \")  # Split into words\n",
    "        if len(new_buffer_words) < min_length:  # Check whether buffer length is too small\n",
    "            chunk_buffer = new_buffer  # Carry over to the next chunk\n",
    "        else:\n",
    "            new_chunks.append(new_buffer)  # Add to chunks\n",
    "            chunk_buffer = \"\"\n",
    "\n",
    "    if len(chunk_buffer) > 0:\n",
    "        new_chunks.append(chunk_buffer)  # Add last chunk, if necessary\n",
    "\n",
    "    return new_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e399530-70aa-4136-9cec-14fbb2412619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mixed_chunks = mixed_chunking(source_text)\n",
    "for i in range(3):\n",
    "    print(f\"Chunk {i+1}: {repr(mixed_chunks[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96929f0e-4425-48cd-b1ca-40c171f38272",
   "metadata": {},
   "source": [
    "This strategy helps ensure that chunks are not too small while still using syntactic markers, like headings, to define boundaries. After examining chunking strategies on one text, let's explore how they perform on a larger collection of texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842cbe38-c1ba-48c5-9df9-df8058424fdc",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4 - Chunking on real data\n",
    "\n",
    "---\n",
    "In this and the following section, there will be comprehensive examples of chunking in practice. You will process several sections of the [Pro Git book](https://git-scm.com/book/en/v2) using different chunking methods and then compare how well each method performs in search tasks.\n",
    "\n",
    "\n",
    "<a id='4-1'></a>\n",
    "### 4.1 Getting the data\n",
    "\n",
    "Let's get the entire 14 chapter book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796e66de-0fd5-4375-b09c-2f765d02381e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_book_text_objects():\n",
    "    # Source location\n",
    "    text_objs = list()\n",
    "    api_base_url = 'https://api.github.com/repos/progit/progit2/contents/book'  # Book base URL\n",
    "    chapter_urls = ['/01-introduction/sections', '/02-git-basics/sections']  # List of section URLs\n",
    "\n",
    "    # Loop through book chapters\n",
    "    for chapter_url in chapter_urls:\n",
    "        response = requests.get(api_base_url + chapter_url)  # Get the JSON data for the section files in the chapter\n",
    "\n",
    "        # Loop through inner files (sections)\n",
    "        for file_info in response.json():\n",
    "            if file_info['type'] == 'file':  # Only process files (not directories)\n",
    "                file_response = requests.get(file_info['download_url'])\n",
    "\n",
    "                # Build objects including metadata\n",
    "                chapter_title = file_info['download_url'].split('/')[-3]\n",
    "                filename = file_info['download_url'].split('/')[-1]\n",
    "                text_obj = {\n",
    "                    \"body\": file_response.text,\n",
    "                    \"chapter_title\": chapter_title,\n",
    "                    \"filename\": filename\n",
    "                }\n",
    "                text_objs.append(text_obj)\n",
    "    return text_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a48242-32e2-4129-936f-928b7c9901d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This will generate a list with 14 elements, one for each chapter\n",
    "book_text_objs = get_book_text_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a89a8e9-a4cf-4a70-b245-47c8c2f1c699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(book_text_objs[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae6e2b0-3153-4265-aa0f-3cc9ac7e55a3",
   "metadata": {},
   "source": [
    "<a id='4-2'></a>\n",
    "### 4.2 Chunking the chapters\n",
    "\n",
    "The following chunking methods will be applied to each section:\n",
    "\n",
    "- **Fixed-length chunks with 20% overlap:**\n",
    "  - Chunks with 25 words each\n",
    "  - Chunks with 100 words each\n",
    "\n",
    "- **Variable-length chunks** using paragraph markers\n",
    "\n",
    "- **Mixed-strategy chunks** using paragraph markers with a minimum chunk length of 25 words\n",
    "\n",
    "Additionally, metadata will be added to each chunk, including the filename, chapter name, and chunk number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd0b33-1208-4658-bd98-9792d7362c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_chunk_objs(book_text_obj, chunks):\n",
    "    \"\"\"\n",
    "    Constructs a list of chunk objects from a given book text object \n",
    "    and its associated chunks.\n",
    "\n",
    "    Args:\n",
    "        book_text_obj (dict): A dictionary containing metadata for the book text, \n",
    "                              including 'chapter_title' and 'filename'.\n",
    "        chunks (list): A list of chunks that represent parts of the book text.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each representing a chunk object \n",
    "              with 'chapter_title', 'filename', 'chunk', and 'chunk_index'.\n",
    "    \"\"\"\n",
    "    chunk_objs = list()  # Initialize an empty list to store chunk objects\n",
    "    \n",
    "    # Iterate over the chunks with an index\n",
    "    for i, c in enumerate(chunks):\n",
    "        # Create a dictionary for each chunk with its associated data\n",
    "        chunk_obj = {\n",
    "            \"chapter_title\": book_text_obj[\"chapter_title\"],  # Chapter title from the book text object\n",
    "            \"filename\": book_text_obj[\"filename\"],            # Filename from the book text object\n",
    "            \"chunk\": c,                                       # The actual chunk of text\n",
    "            \"chunk_index\": i                                  # The index of the chunk in the list\n",
    "        }\n",
    "        # Append the constructed chunk object to the list\n",
    "        chunk_objs.append(chunk_obj)\n",
    "\n",
    "    # Return the list of chunk objects\n",
    "    return chunk_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e056c1d-be28-4829-bc5c-620e3558f822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get multiple sets of chunks - according to chunking strategy\n",
    "chunk_obj_sets = dict()\n",
    "for book_text_obj in book_text_objs:\n",
    "    text = book_text_obj[\"body\"]  # Get the object's text body\n",
    "\n",
    "    # Loop through chunking strategies:\n",
    "    for strategy_name, chunks in [\n",
    "        [\"fixed_size_25\", get_chunks_fixed_size_with_overlap(text, 25, 0.2)],\n",
    "        [\"fixed_size_100\", get_chunks_fixed_size_with_overlap(text, 100, 0.2)],\n",
    "        [\"para_chunks\", get_chunks_by_paragraph(text)],\n",
    "        [\"para_chunks_min_25\", mixed_chunking(text)]\n",
    "    ]:\n",
    "        chunk_objs = build_chunk_objs(book_text_obj, chunks)\n",
    "\n",
    "        if strategy_name not in chunk_obj_sets.keys():\n",
    "            chunk_obj_sets[strategy_name] = list()\n",
    "\n",
    "        chunk_obj_sets[strategy_name] += chunk_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b7a3d0-667a-4af1-ab13-785db626f623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chunk_obj_sets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669438c2-038a-497f-a7a9-03a871d6fbd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_type = 'fixed_size_25' # Change it to check the different chunks!\n",
    "chunk_obj_sets[chunk_type][0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4672de46-f128-484a-ae2c-8077bf39f31e",
   "metadata": {},
   "source": [
    "<a id='4-3'></a>\n",
    "### 4.3 Loading Chunks into a Vector Database\n",
    "\n",
    "In this section, you'll focus on loading chunks into a vector database. Below, you'll find an outline of how to create and load data into the vector database. However, in this lab, you will work with a pre-loaded collection to save time. If you haven't yet completed the ungraded lab on the Weaviate API, it's highly recommended you do so for a better understanding of the process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607aeb81-d43a-4adf-a64d-9a9c50864581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the client\n",
    "with suppress_subprocess_output():\n",
    "    try:\n",
    "        client = weaviate.connect_to_embedded(\n",
    "            persistence_data_path=\"/home/jovyan/data/collections/m3/ungraded_lab_2\",\n",
    "            environment_variables={\n",
    "                \"ENABLE_API_BASED_MODULES\": \"true\", # Enable API based modules \n",
    "                \"ENABLE_MODULES\": 'text2vec-transformers', # We will be using a transformer model \n",
    "                \"TRANSFORMERS_INFERENCE_API\":\"http://127.0.0.1:5000/\", # The endpoint the weaviate API will be using to vectorize\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        ports = extract_ports(str(e))\n",
    "        client = weaviate.connect_to_local(port=8079, grpc_port=50050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f4dd6-316c-4275-b207-d4b0046b97c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating the collection\n",
    "if not client.collections.exists(\"chunking_example\"):\n",
    "    collection = client.collections.create(\n",
    "            name='chunking_example',\n",
    "\n",
    "            vectorizer_config=[Configure.NamedVectors.text2vec_transformers(\n",
    "                    name=\"vector\", # This is the name you will need to access the vectors of the objects in your collection\n",
    "                    #source_properties=['chunk'], # which properties should be used to generate a vector, they will be appended to each other when vectorizing\n",
    "                    vectorize_collection_name = False, # This tells the client to not vectorize the collection name. \n",
    "                                                       # If True, it will be appended at the beginning of the text to be vectorized\n",
    "                    inference_url=\"http://127.0.0.1:5000\", # Since we are using an API based vectorizer, you need to pass the URL used to make the calls \n",
    "                                                           # This was setup in our Flask application\n",
    "                )],\n",
    "\n",
    "            properties=[  # Define properties\n",
    "            Property(name=\"chunk\",data_type= DataType.TEXT),\n",
    "            Property(name=\"chapter_title\", data_type=DataType.TEXT),\n",
    "            Property(name=\"filename\",data_type=DataType.TEXT),\n",
    "            Property(name=\"chunking_strategy\",data_type=DataType.TEXT, tokenization = Tokenization.FIELD), # tokenization = Tokenization.FIELD means that the entire word will be treated as a token,\n",
    "            Property(name=\"chunk_index\",data_type=DataType.INT),\n",
    "\n",
    "        ]\n",
    "        )\n",
    "else:\n",
    "    collection = client.collections.get(\"chunking_example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a318d8c2-c6a2-4881-b8e6-812ec5122749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adding elements in the collection - this insertion should NOT run as the collection is already vectorized for you. \n",
    "if len(collection) == 0:\n",
    "    with collection.batch.fixed_size(batch_size=1, concurrent_requests=1) as batch:\n",
    "        for chunking_strategy, chunk_objects in tqdm.tqdm(chunk_obj_sets.items()):\n",
    "            for chunk_obj in chunk_objects:\n",
    "                chunk_obj[\"chunking_strategy\"] = chunking_strategy\n",
    "                batch.add_object(\n",
    "                    properties=chunk_obj,\n",
    "                    uuid=generate_uuid5(chunk_obj)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9d19a5-41f2-4552-a5dd-9a54341cf349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Total count: {collection.aggregate.over_all().total_count}\")\n",
    "for chunking_strategy in chunk_obj_sets.keys():\n",
    "    where_filter = Filter.by_property('chunking_strategy').equal(chunking_strategy) # Filter by chunking strategy\n",
    "    count = collection.aggregate.over_all(filters = where_filter).total_count # Aggregate with filtering\n",
    "    print(f\"Object count for {chunking_strategy}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992bd721-9551-41ce-94c6-13f5870f1350",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5 - Searching \n",
    "---\n",
    "In this section, you will explore semantic searching with different chunk sizes to visualize the impacts of the sizes in information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e92e362-2f1a-4261-8426-9447704fe697",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_string = \"history of git\"  # Or \"available git remote commands\"\n",
    "\n",
    "for chunking_strategy in chunk_obj_sets.keys():\n",
    "    where_filter = Filter.by_property('chunking_strategy').equal(chunking_strategy)\n",
    "    response = collection.query.near_text(search_string, filters = where_filter, limit = 2)\n",
    "    print(f\"RETRIEVED OBJECTS FOR CHUNKING STRATEGY {chunking_strategy.upper()}:\\n\")\n",
    "    for i, obj in enumerate(response.objects):\n",
    "        print(f\"===== Object {i} =====\")\n",
    "        print(f\"{obj.properties['chunk']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9705e2-8842-4340-9f47-03088f491f8c",
   "metadata": {},
   "source": [
    "In this example, the query is a broad one focused on the \"history of git.\" The results show that longer chunks tend to perform better. Upon examination, while the 25-word chunks might closely match the query in terms of semantic similarity, they lack sufficient context to significantly enhance the reader's understanding of the topic. Conversely, the paragraph chunks retrieved—particularly those with a minimum length of 25 words—provide comprehensive information that effectively educates the reader about the history of Git."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f223b7-225e-4936-b045-3dbcd32eae14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_string = \"how to add the url of a remote repository\"  # Or \"available git remote commands\"\n",
    "\n",
    "for chunking_strategy in chunk_obj_sets.keys():\n",
    "    where_filter = Filter.by_property('chunking_strategy').equal(chunking_strategy)\n",
    "    response = collection.query.near_text(search_string, filters = where_filter, limit = 2)\n",
    "    print(f\"RETRIEVED OBJECTS FOR CHUNKING STRATEGY {chunking_strategy.upper()}:\\n\")\n",
    "    for i, obj in enumerate(response.objects):\n",
    "        print(f\"===== Object {i} =====\")\n",
    "        print(f\"{obj.properties['chunk']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb5e9cc-a5ae-4a0a-8256-95bcc0688768",
   "metadata": {},
   "source": [
    "In this example, the query was more specific, such as one made by a user looking to find out how to add the URL of a remote repository. Unlike the previous scenario, the 25-word chunks prove more useful here. Because the question was very specific, Weaviate could pinpoint the chunk with the most relevant passage—how to add a remote repository (`git remote add <shortname> <url>`). \n",
    "\n",
    "Although other result sets contain some of this information, it's important to consider how the result will be used and displayed. Longer results might require more cognitive effort from the user to extract the relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69016bf6-5a10-4da9-9314-2272e8d9e70c",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6 - Incorporating in a RAG system\n",
    "---\n",
    "Now you are familiar with chunking and you have a fully working collection, let's see how different chunk sizes impact text generation. Let's use a simple prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291edd5f-0561-4e85-8431-e292498ead23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROMPT = \"Using this information and only this information, please explain {search_string} in a few short points.\\nContext: {context}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2070c20-68fa-4a45-bc06-2b47f6a53f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set number of chunks to retrieve to compensate for different chunk sizes\n",
    "\n",
    "n_chunks_by_strat = dict()\n",
    "\n",
    "# Grab more of shorter chunks\n",
    "n_chunks_by_strat['fixed_size_25'] = 8\n",
    "n_chunks_by_strat['para_chunks'] = 8\n",
    "\n",
    "# Grab fewer of longer chunks\n",
    "n_chunks_by_strat['fixed_size_100'] = 2\n",
    "n_chunks_by_strat['para_chunks_min_25'] = 2\n",
    "\n",
    "# Perform Retreval augmented generation\n",
    "search_string = \"history of git\"  # Or \"available git remote commands\"\n",
    "\n",
    "for chunking_strategy in chunk_obj_sets.keys():\n",
    "    where_filter = Filter.by_property('chunking_strategy').equal(chunking_strategy)\n",
    "    response = collection.query.near_text(search_string, filters = where_filter, limit = n_chunks_by_strat[chunking_strategy])\n",
    "    context_string = \"\"\n",
    "    for obj in response.objects:\n",
    "        context_string += obj.properties['chunk'] + '\\n'\n",
    "    prompt = PROMPT.format(search_string = search_string, context = context_string)\n",
    "    response = generate_with_single_input(prompt, role = 'assistant')\n",
    "    print(f\"Search string: {search_string}\")\n",
    "    print(f\"Chunking Strategy: {chunking_strategy}:\")\n",
    "    print(f\"Response:\\n\\t{response['content']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf43ec-7702-4fce-9eff-4b2ffab0b7d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Don't forget to close the client!\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14490e4e-859f-4d5d-a1ec-07f637a55268",
   "metadata": {},
   "source": [
    "Congratulations! You've finished the ungraded lab on Chunking! Keep it up!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
